{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad322f54-44ec-4c2c-a85a-8bce5d7cc11e",
   "metadata": {},
   "source": [
    "# **Semantic Document Analysis with OpenAI, Pinecone, and LangChain**  \n",
    "## **Overview**  \n",
    "\n",
    "This Jupyter notebook provides a  tool for performing **semantic search** across a collection of **PDF document**.\n",
    "It's designed to help researchers, analysts, and students efficiently find conceptually related information without relying on exact keyword matches.\n",
    "Leveraging **Pinecone** for vector storage, **OpenAI embeddings** for generating dense semantic vectors, the notebook enables advanced extraction, indexing, and querying of document content.\n",
    "By leveraging state-of-the-art AI models and a vector database, this notebook enables **semantic reading** and transforms your static PDFs into a fully searchable, intelligent knowledge base.\n",
    "\n",
    "---\n",
    "\n",
    "### **Technical Approach**\n",
    "\n",
    "#### **Dense Vector Semantic Search**\n",
    "- Uses **OpenAI text-embedding-3-large** model (3072 dimensions) \n",
    "- Performs **nearest neighbor search** in high-dimensional vector space\n",
    "- Enables **similarity search** based on semantic meaning and context\n",
    "- Excellent for finding conceptually related content even with different wording\n",
    "- **Limitation**: May miss exact keyword matches, especially domain-specific terms\n",
    "\n",
    "### **Key Features**  \n",
    "\n",
    "#### **Document Processing**  \n",
    "- Extracts text from **PDF documents** while maintaining **page-level references**.  \n",
    "- Handles files with **year-based naming conventions** (e.g., \"1946-document-name.pdf\").  \n",
    "- Skips unwanted pages (e.g., covers, introductions) for focused analysis.  \n",
    "- Splits text into **manageable chunks** for efficient indexing and search.  \n",
    "\n",
    "#### **Semantic Search**  \n",
    "- Uses **OpenAI embeddings** to enable **semantic understanding** of queries.  \n",
    "- Stores processed documents in **Pinecone**, a vector database, for fast and scalable search.  \n",
    "- Supports **customizable queries** to find relevant passages based on meaning, not just keywords.  \n",
    "\n",
    "#### **Analysis & Output**  \n",
    "- Extracts **contextual excerpts** around matching terms or concepts.  \n",
    "- Preserves **metadata** such as source file, page number, and publication year.  \n",
    "- Outputs results in a **readable format** for both console and file saving.  \n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works**  \n",
    "1. **Document Extraction**: Text is extracted from PDFs, skipping specified pages and splitting content into chunks.  \n",
    "2. **Indexing**: Each text chunk is vectorized and stored in Pinecone for fast retrieval.  \n",
    "3. **Querying**: Users can search for specific terms, phrases, or concepts, and the tool retrieves semantically similar results.  \n",
    "4. **Context Preservation**: Matching passages are displayed with surrounding context for better understanding.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Getting Started**  \n",
    "1. Install the required Python libraries.  \n",
    "2. Upload your **PDF documents** to the specified directory.  \n",
    "3. Configure your **API keys** for Pinecone and OpenAI when prompted.  \n",
    "4. Run the notebook to process, index, and query your documents.  \n",
    "\n",
    "---\n",
    "\n",
    "# **Pinecone and OpenAI Embeddings**\n",
    "\n",
    "This Jupyter notebook demonstrates how to use **Pinecone** and **OpenAI embeddings** to process and search PDF documents. Students will learn how to:\n",
    "\n",
    "1. Set up Pinecone and OpenAI APIs.\n",
    "2. Extract and split text from PDFs.\n",
    "3. Store and query embeddings in Pinecone.\n",
    "4. Modify parameters like `top_k` and query text to experiment with semantic search results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea78fb0-bf52-45f1-a7f6-21e3219b591c",
   "metadata": {},
   "source": [
    "## **Setup**\n",
    "\n",
    "First, install the required libraries and set up your API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47391fc3-e2a0-478c-8173-2ef91fdf7723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pinecone langchain-pinecone langchain-openai langchain openai pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3daf829-8cb8-47ba-80de-d13085539aaf",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902db1c1-3f59-4d66-b8e6-ac562bc764f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import hashlib\n",
    "import re\n",
    "import fitz  # For PDF processing\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore  # Updated import\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0a7dfb-4c79-4809-b74f-596e4993bdd3",
   "metadata": {},
   "source": [
    "## **Step 3: API Key Configuration**\n",
    "\n",
    "This step configures the API keys required for Pinecone (vector database) and OpenAI (embeddings). Both services require free account registration and API key generation.\n",
    "\n",
    "### **üîê Required API Keys**\n",
    "\n",
    "You'll need API keys from two services:\n",
    "1. **Pinecone** - For vector database storage and retrieval\n",
    "2. **OpenAI** - For text embedding generation\n",
    "\n",
    "### **üìã Pinecone API Key Setup**\n",
    "\n",
    "#### **What is Pinecone?**\n",
    "Pinecone is a vector database service that stores and searches high-dimensional vectors efficiently. It's perfect for semantic search applications.\n",
    "\n",
    "#### **How to get your Pinecone API Key:**\n",
    "\n",
    "1. **Sign Up for Pinecone**\n",
    "   - Visit: [https://www.pinecone.io/](https://www.pinecone.io/)\n",
    "   - Click **\"Sign Up\"** (free tier available)\n",
    "   - Create your account with email verification\n",
    "\n",
    "2. **Access API Keys**\n",
    "   - Log into your [Pinecone Console](https://app.pinecone.io/)\n",
    "   - Navigate to **\"API Keys\"** in the left sidebar\n",
    "   - Click **\"Create API Key\"**\n",
    "\n",
    "3. **Copy Your API Key**\n",
    "   - Copy the generated API key (starts with `pc-...`)\n",
    "   - **‚ö†Ô∏è Important**: Save it securely - you won't see it again!\n",
    "\n",
    "4. **Choose Your Environment**\n",
    "   - Default region: `us-east-1` (recommended for beginners)\n",
    "   - Other options: `us-west-2`, `eu-west-1`, etc.\n",
    "\n",
    "#### **üí° Pinecone Free Tier Includes:**\n",
    "- 1 project\n",
    "- 1 index\n",
    "- 5M vector dimensions\n",
    "- Enough for this tutorial and small projects\n",
    "\n",
    "---\n",
    "\n",
    "### **ü§ñ OpenAI API Key Setup**\n",
    "\n",
    "#### **What is OpenAI API?**\n",
    "OpenAI's API provides access to powerful language models including embedding models that convert text into numerical vectors for semantic search.\n",
    "\n",
    "#### **How to get your OpenAI API Key:**\n",
    "\n",
    "1. **Sign Up for OpenAI**\n",
    "   - Visit: [https://platform.openai.com/](https://platform.openai.com/)\n",
    "   - Click **\"Sign up\"** \n",
    "   - Create account or sign in\n",
    "\n",
    "2. **Access API Section**\n",
    "   - Go to [OpenAI API Platform](https://platform.openai.com/api-keys)\n",
    "   - Or navigate to **\"API\"** ‚Üí **\"API keys\"**\n",
    "\n",
    "3. **Create New API Key**\n",
    "   - Click **\"+ Create new secret key\"**\n",
    "   - Give it a descriptive name (e.g., \"Text Analysis Tool\")\n",
    "   - Copy the key (starts with `sk-...`)\n",
    "   - **‚ö†Ô∏è Critical**: Store securely - this is shown only once!\n",
    "\n",
    "4. **Set Up Billing (Required)**\n",
    "   - Go to [Billing Settings](https://platform.openai.com/account/billing)\n",
    "   - Add a payment method\n",
    "   - Set usage limits to control costs\n",
    "   - **üí∞ Cost**: Embedding API is very\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6ee89e-bac3-49a7-8d1b-a4fd61e53104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key Configuration with User Input\n",
    "def get_api_keys():\n",
    "    \"\"\"Get API keys from environment variables or user input.\"\"\"\n",
    "    api_keys = {}\n",
    "    \n",
    "    # Get Pinecone API key\n",
    "    # Get your free API key from: https://www.pinecone.io/\n",
    "    pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "    if not pinecone_api_key:\n",
    "        pinecone_api_key = input(\"Enter your Pinecone API key: \").strip()\n",
    "    \n",
    "    # Get OpenAI API key  \n",
    "    # Get your API key from: https://platform.openai.com/api-keys\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not openai_api_key:\n",
    "        openai_api_key = input(\"Enter your OpenAI API key: \").strip()\n",
    "    \n",
    "    # Get Pinecone Environment (optional, defaults to us-east-1)\n",
    "    pinecone_env = os.getenv('PINECONE_ENV')\n",
    "    if not pinecone_env:\n",
    "        pinecone_env = input(\"Enter your Pinecone environment (default: us-east-1): \").strip()\n",
    "        if not pinecone_env:\n",
    "            pinecone_env = \"us-east-1\"\n",
    "    \n",
    "    # Validate API keys\n",
    "    if pinecone_api_key and openai_api_key:\n",
    "        api_keys['pinecone'] = pinecone_api_key\n",
    "        api_keys['openai'] = openai_api_key\n",
    "        api_keys['pinecone_env'] = pinecone_env\n",
    "        print(\"üîë All API keys configured!\")\n",
    "        return api_keys\n",
    "    else:\n",
    "        print(\"‚ùå Need both Pinecone and OpenAI API keys to continue.\")\n",
    "        print(\"Get Pinecone API key from: https://www.pinecone.io/\")\n",
    "        print(\"Get OpenAI API key from: https://platform.openai.com/api-keys\")\n",
    "        return None\n",
    "\n",
    "# Get API keys\n",
    "api_keys = get_api_keys()\n",
    "\n",
    "if api_keys:\n",
    "    PINECONE_API_KEY = api_keys['pinecone']\n",
    "    OPENAI_API_KEY = api_keys['openai']\n",
    "    PINECONE_ENV = api_keys['pinecone_env']\n",
    "    print(\"üöÄ Text Pattern Analysis Tool ready!\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot continue without API keys.\")\n",
    "    raise Exception(\"API keys required to continue\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3510eb70-6c50-4771-a3b4-d44b0fc48801",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Pinecone and OpenAI embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5303de1c-7cfc-454d-84a2-eddc26f2a2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Initialize Pinecone\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    \n",
    "    # Initialize OpenAI embeddings\n",
    "    model_name = 'text-embedding-3-large'  # 3072 dimensions\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=model_name)\n",
    "    \n",
    "    # Set up the index name\n",
    "    index_name = \"tdm-test3\"\n",
    "    \n",
    "    print(\"üîß Initializing Pinecone and OpenAI embeddings...\")\n",
    "    print(f\"üìä Using OpenAI model: {model_name}\")\n",
    "    print(f\"üóÇÔ∏è Pinecone index: {index_name}\")\n",
    "    print(\"‚úÖ Initialization successful!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing services: {str(e)}\")\n",
    "    print(\"Please check your API keys and try again.\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b24e137-778e-42d9-a363-74f0542688ef",
   "metadata": {},
   "source": [
    "## Step 5: Process PDFs\n",
    "This section extracts text from PDFs, splits it into chunks, and prepares it for embedding.\n",
    "- **‚ö†Ô∏è PDF size Warning**: Indexing large PDF files may take a while. For testing purposes, please upload smaller files (preferably under **50 MB**).\n",
    "\n",
    "To create your first index, enter the directory path containing your PDFs (e.g., **./rag-input**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccaec94-ebf4-4607-a42c-d7e67bb68a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the index exists and create if necessary\n",
    "try:\n",
    "    existing_indexes = pc.list_indexes().names()\n",
    "    \n",
    "    if index_name in existing_indexes:\n",
    "        print(f\"‚úÖ Index '{index_name}' already exists. Connecting to existing index...\")\n",
    "        # Initialize vector store directly with existing index\n",
    "        vector_store = PineconeVectorStore(\n",
    "            index_name=index_name,\n",
    "            embedding=embeddings,\n",
    "            pinecone_api_key=PINECONE_API_KEY\n",
    "        )\n",
    "        # Get index reference for direct querying\n",
    "        index = pc.Index(index_name)\n",
    "        print(f\"‚úÖ Connected to existing index '{index_name}'\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"üÜï Index '{index_name}' does not exist. Creating new index...\")\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=3072,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(\n",
    "                cloud='aws',\n",
    "                region=PINECONE_ENV\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Index '{index_name}' created successfully!\")\n",
    "        print(\"‚è≥ Waiting for index to be ready...\")\n",
    "        \n",
    "        # Wait for index to be ready\n",
    "        import time\n",
    "        time.sleep(10)\n",
    "        \n",
    "        # Initialize Pinecone vector store with the new approach\n",
    "        vector_store = PineconeVectorStore(\n",
    "            index_name=index_name,\n",
    "            embedding=embeddings,\n",
    "            pinecone_api_key=PINECONE_API_KEY\n",
    "        )\n",
    "        \n",
    "        # Get index reference for direct querying\n",
    "        index = pc.Index(index_name)\n",
    "\n",
    "        # Initialize text splitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1500,\n",
    "            chunk_overlap=500\n",
    "        )\n",
    "\n",
    "        # Define the starting pages offsets for specific filenames\n",
    "        file_start_pages = {\n",
    "            '1946-jahrbuch-des-lutherbundes.pdf': 5,  # Skip first 5 pages\n",
    "            '1947-jahrbuch-des-lutherbundes.pdf': 4,\n",
    "            '1948-jahrbuch-des-lutherbundes.pdf': 5,\n",
    "        }\n",
    "\n",
    "        def extract_text_from_pdf(pdf_path):\n",
    "            \"\"\"Extract text and basic page numbers from PDF.\"\"\"\n",
    "            pdf_document = fitz.open(pdf_path)\n",
    "            pdf_text = {}\n",
    "            page_numbers = {}\n",
    "            \n",
    "            filename = os.path.basename(pdf_path)\n",
    "            skip_pages = file_start_pages.get(filename, 0)\n",
    "            \n",
    "            # Start counting from after the skipped pages\n",
    "            for page_num in range(pdf_document.page_count):\n",
    "                page = pdf_document.load_page(page_num)\n",
    "                text = page.get_text().strip()\n",
    "                \n",
    "                if text:\n",
    "                    if page_num >= skip_pages:\n",
    "                        adjusted_page_num = page_num - skip_pages + 1\n",
    "                        pdf_text[adjusted_page_num] = text\n",
    "                        page_numbers[adjusted_page_num] = adjusted_page_num\n",
    "            \n",
    "            pdf_document.close()\n",
    "            return pdf_text, page_numbers\n",
    "\n",
    "        def process_pdfs_in_directory(directory_path):\n",
    "            \"\"\"Process all PDFs in directory with duplicate prevention.\"\"\"\n",
    "            documents = []\n",
    "            processed_chunks = set()\n",
    "            \n",
    "            pdf_files = [f for f in os.listdir(directory_path) if f.endswith(\".pdf\")]\n",
    "            \n",
    "            if not pdf_files:\n",
    "                print(f\"‚ö†Ô∏è No PDF files found in {directory_path}\")\n",
    "                return documents\n",
    "                \n",
    "            for filename in pdf_files:\n",
    "                pdf_path = os.path.join(directory_path, filename)\n",
    "                print(f\"üìÑ Processing {filename}...\")\n",
    "                \n",
    "                try:\n",
    "                    pdf_text, page_numbers = extract_text_from_pdf(pdf_path)\n",
    "                    \n",
    "                    if not pdf_text:\n",
    "                        print(f\"‚ö†Ô∏è No text extracted from {filename}\")\n",
    "                        continue\n",
    "                    \n",
    "                    chunk_count = 0\n",
    "                    for page_num, text in pdf_text.items():\n",
    "                        chunks = text_splitter.split_text(text)\n",
    "                        \n",
    "                        for i, chunk in enumerate(chunks):\n",
    "                            chunk_content = chunk.strip()\n",
    "                            if chunk_content and chunk_content not in processed_chunks:\n",
    "                                processed_chunks.add(chunk_content)\n",
    "                                \n",
    "                                year = filename[:4] if filename[:4].isdigit() else \"Unknown\"\n",
    "                                \n",
    "                                doc = Document(\n",
    "                                    page_content=chunk,\n",
    "                                    metadata={\n",
    "                                        \"source\": filename,\n",
    "                                        \"page\": page_num,\n",
    "                                        \"chunk\": i + 1,\n",
    "                                        \"publishYear\": year,\n",
    "                                        \"chunk_id\": f\"{filename}_{page_num}_{i}_{hash(chunk)}\"\n",
    "                                    }\n",
    "                                )\n",
    "                                documents.append(doc)\n",
    "                                chunk_count += 1\n",
    "                    \n",
    "                    print(f\"‚úÖ Processed {filename}: {chunk_count} unique chunks created\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error processing {filename}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            return documents\n",
    "\n",
    "        def process_and_index_pdfs():\n",
    "            \"\"\"Main function to process and index PDFs.\"\"\"\n",
    "            pdf_directory = input(\"Enter the path to your PDF directory:e.g. './rag-input' \").strip()\n",
    "            \n",
    "            if not pdf_directory:\n",
    "                pdf_directory = \"./rag-input\"\n",
    "                print(f\"Using default directory: {pdf_directory}\")\n",
    "            \n",
    "            if not os.path.exists(pdf_directory):\n",
    "                print(f\"‚ùå Directory '{pdf_directory}' does not exist!\")\n",
    "                return False\n",
    "            \n",
    "            try:\n",
    "                print(\"üîÑ Processing PDFs...\")\n",
    "                documents = process_pdfs_in_directory(pdf_directory)\n",
    "                \n",
    "                if documents:\n",
    "                    print(f\"\\nüì§ Starting to add {len(documents)} documents to Pinecone...\")\n",
    "                    batch_size = 100\n",
    "                    for i in range(0, len(documents), batch_size):\n",
    "                        batch = documents[i:i + batch_size]\n",
    "                        vector_store.add_documents(batch)\n",
    "                        print(f\"üì¶ Added batch {i//batch_size + 1}/{(len(documents)-1)//batch_size + 1} to Pinecone\")\n",
    "                    \n",
    "                    print(f\"\\n‚úÖ Successfully processed and stored {len(documents)} unique chunks in Pinecone.\")\n",
    "                    \n",
    "                    print(\"\\nüìã Sample of processed documents:\")\n",
    "                    for doc in documents[:2]:\n",
    "                        print(f\"\\nMetadata: {doc.metadata}\")\n",
    "                        print(f\"Content preview: {doc.page_content[:100]}...\")\n",
    "                    return True\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è No documents were processed.\")\n",
    "                    return False\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå An error occurred: {str(e)}\")\n",
    "                return False\n",
    "\n",
    "        # Run the processing for new index\n",
    "        process_and_index_pdfs()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error with Pinecone operations: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b328a0-d7b7-4083-98e6-9aed95a05077",
   "metadata": {},
   "source": [
    "## Step 6: Query Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe7ece-12af-4c68-b754-914a4fdb5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pinecone(query_text, top_k=10, output_dir=\"output\"):\n",
    "    \"\"\"Query Pinecone index for similar results.\"\"\"\n",
    "    try:\n",
    "        print(f\"üîç Searching for: '{query_text}'\")\n",
    "        \n",
    "        # Use vector store similarity search for better integration\n",
    "        results = vector_store.similarity_search_with_score(\n",
    "            query=query_text,\n",
    "            k=top_k\n",
    "        )\n",
    "        \n",
    "        seen_results = set()\n",
    "        unique_matches = []\n",
    "        \n",
    "        for doc, score in results:\n",
    "            content = doc.page_content.strip()\n",
    "            if content and content not in seen_results:\n",
    "                seen_results.add(content)\n",
    "                # Convert to match format for compatibility\n",
    "                match = {\n",
    "                    'score': score,\n",
    "                    'metadata': {\n",
    "                        'text': content,\n",
    "                        **doc.metadata\n",
    "                    }\n",
    "                }\n",
    "                unique_matches.append(match)\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save query results\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        query_results_file = os.path.join(output_dir, f\"query_results_{timestamp}.txt\")\n",
    "        with open(query_results_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Query: {query_text}\\n\")\n",
    "            f.write(f\"Top K: {top_k}\\n\")\n",
    "            f.write(f\"Results found: {len(unique_matches)}\\n\\n\")\n",
    "            for i, match in enumerate(unique_matches, 1):\n",
    "                f.write(f\"Result {i}:\\n\")\n",
    "                f.write(f\"Score: {match['score']}\\n\")\n",
    "                f.write(f\"Content: {match['metadata'].get('text', '')}\\n\")\n",
    "                f.write(f\"Metadata: {match['metadata']}\\n\")\n",
    "                f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        print(f\"üíæ Query results saved to {query_results_file}\")\n",
    "        return unique_matches\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during query: {str(e)}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15407975-a93b-4a04-a6ba-74a59dae950b",
   "metadata": {},
   "source": [
    "## Step 7: Display Query Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d448088e-63b1-40b4-8241-f58b6e1b5ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSI escape codes for colors\n",
    "RED = \"\\033[91m\"\n",
    "RESET = \"\\033[0m\"\n",
    "BOLD = \"\\033[1m\"\n",
    "\n",
    "def find_context(full_text, context_size=600):\n",
    "    \"\"\"Find context around matching text.\"\"\"\n",
    "    total_length = len(full_text)\n",
    "    \n",
    "    if total_length <= context_size * 3:\n",
    "        part_size = total_length // 3\n",
    "        return full_text[:part_size], full_text[part_size:2*part_size], full_text[2*part_size:]\n",
    "    \n",
    "    middle = total_length // 2\n",
    "    match_start = max(0, middle - context_size // 2)\n",
    "    match_end = min(total_length, match_start + context_size)\n",
    "    matching = full_text[match_start:match_end]\n",
    "    \n",
    "    before_start = max(0, match_start - context_size)\n",
    "    before = full_text[before_start:match_start]\n",
    "    \n",
    "    after_end = min(total_length, match_end + context_size)\n",
    "    after = full_text[match_end:after_end]\n",
    "    \n",
    "    return before, matching, after\n",
    "\n",
    "def format_context(before, matching, after):\n",
    "    return f\"...{RESET}{before}{BOLD}{RED}{matching}{RESET}{after}...\"\n",
    "\n",
    "def format_context_for_file(before, matching, after):\n",
    "    return f\"...{before}<<< {matching} >>>{after}...\"\n",
    "\n",
    "def display_query_results(results):\n",
    "    \"\"\"Display query results in a readable format.\"\"\"\n",
    "    if not results:\n",
    "        print(\"‚ùå No results to display.\")\n",
    "        return None\n",
    "        \n",
    "    console_output = []\n",
    "    file_output = []\n",
    "    seen_results = set()\n",
    "    \n",
    "    print(f\"\\nüìä Displaying {len(results)} results:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, match in enumerate(results, 1):\n",
    "        result_key = match['metadata'].get('text', '').strip()\n",
    "        if result_key and result_key not in seen_results:\n",
    "            seen_results.add(result_key)\n",
    "            \n",
    "            source = match['metadata'].get('source', 'Unknown')\n",
    "            page = match['metadata'].get('page', 'Unknown')\n",
    "            publish_year = match['metadata'].get('publishYear', 'Unknown')\n",
    "            chunk = match['metadata'].get('chunk', 'Unknown')\n",
    "            score = match['score']\n",
    "            \n",
    "            full_text = match['metadata'].get('text', '')\n",
    "            before, matching, after = find_context(full_text)\n",
    "            formatted_text = format_context(before, matching, after)\n",
    "            \n",
    "            console_output.append(f\"üîé Result {i}:\")\n",
    "            console_output.append(f\"üìà Score: {score:.4f}\")\n",
    "            console_output.append(f\"üìÑ Source: {source}\")\n",
    "            console_output.append(f\"üìã Page: {page}\")\n",
    "            console_output.append(f\"üìÖ Publish Year: {publish_year}\")\n",
    "            console_output.append(f\"üì¶ Chunk: {chunk}\")\n",
    "            console_output.append(formatted_text)\n",
    "            console_output.append(\"-\" * 50 + \"\\n\")\n",
    "            \n",
    "            file_output.append(f\"Result {i}:\")\n",
    "            file_output.append(f\"Score: {score:.4f}\")\n",
    "            file_output.append(f\"Source: {source}\")\n",
    "            file_output.append(f\"Page: {page}\")\n",
    "            file_output.append(f\"Publish Year: {publish_year}\")\n",
    "            file_output.append(f\"Chunk: {chunk}\")\n",
    "            file_text = format_context_for_file(before, matching, after)\n",
    "            file_output.append(file_text)\n",
    "            file_output.append(\"-\" * 50 + \"\\n\")\n",
    "    \n",
    "    print(\"\\n\".join(console_output))\n",
    "    \n",
    "    output_dir = \"rag-output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"formatted_results_{timestamp}.txt\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(file_output))\n",
    "    \n",
    "    print(f\"üíæ Formatted results saved to {filepath}\")\n",
    "    return filepath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b7e94c-c5a2-4e48-85cd-acf356ffb493",
   "metadata": {},
   "source": [
    "## Step 8: Interactive Query Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb43e17-8e48-4cca-84e2-fe915e008097",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def interactive_query():\n",
    "    \"\"\"Interactive function to query the database.\"\"\"\n",
    "    print(\"\\nüéØ Interactive Query Mode\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    while True:\n",
    "        query_text = input(\"\\nEnter your search query (or 'quit' to exit): \").strip()\n",
    "        \n",
    "        if query_text.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"üëã Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        if not query_text:\n",
    "            print(\"‚ö†Ô∏è Please enter a valid query.\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            top_k_input = input(\"Enter number of results (default 10): \").strip()\n",
    "            top_k = int(top_k_input) if top_k_input else 10\n",
    "        except ValueError:\n",
    "            top_k = 10\n",
    "            print(\"‚ö†Ô∏è Invalid number, using default (10)\")\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\nüîÑ Searching...\")\n",
    "            results = query_pinecone(query_text, top_k=top_k)\n",
    "            if results:\n",
    "                display_query_results(results)\n",
    "            else:\n",
    "                print(\"‚ùå No results found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during search: {str(e)}\")\n",
    "\n",
    "# Run the interactive query\n",
    "print(\"\\nüöÄ Setup complete! Ready to search your documents.\")\n",
    "interactive_query()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ce5eff-1adf-47de-a195-af2bf9bc37cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
