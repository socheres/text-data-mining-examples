{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cd3aeaa-4b4b-4e10-b2aa-8602bc3c48b3",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER) for PDF Documents Analysis\n",
    "\n",
    "This Jupyter notebook implements a comprehensive Named Entity Recognition (NER) system specifically designed for analyzing PDF documents. The system focuses on extracting and analyzing person names (PER entities) from documents, with particular optimization for German language texts.\n",
    "\n",
    "## Key Features:\n",
    "\n",
    "1. **Multi-Language Support**:\n",
    "   - Handles English, German, French, and Spanish texts\n",
    "   - Automatically detects document language\n",
    "   - Uses specialized language models for accurate entity recognition\n",
    "\n",
    "2. **PDF Processing**:\n",
    "   - Extracts text from PDF documents\n",
    "   - Handles both searchable PDFs and scanned documents\n",
    "   - Processes documents in chunks to manage large files\n",
    "\n",
    "3. **Entity Processing**:\n",
    "   - Focuses on person name extraction (PER entities)\n",
    "   - Implements intelligent name normalization\n",
    "   - Filters out false positives\n",
    "   - Provides context for extracted entities\n",
    "\n",
    "4. **Visualization Features**:\n",
    "   - Creates bar plots of top entities\n",
    "   - Generates word clouds\n",
    "   - Provides year-wise analysis for temporal insights\n",
    "   - Visualizes entity distribution across documents\n",
    "\n",
    "5. **Output Generation**:\n",
    "   - Produces detailed Excel reports\n",
    "   - Creates summary sheets with entity counts\n",
    "   - Generates year-specific analysis\n",
    "   - Includes context for each extracted entity\n",
    "\n",
    "This notebook is particularly useful for researchers and analysts working with historical documents or large collections of PDF files who need to extract and analyze named entities, especially person names, with a focus on accuracy and context preservation.\n",
    "\n",
    "The system includes robust error handling, progress tracking, and detailed output formatting, making it suitable for both small-scale and large-scale document analysis projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae755ab-a1af-4b19-8104-f11c726ebce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install Required Libraries\n",
    "\"\"\"\n",
    "# Named Entity Recognition (NER) for PDF Documents\n",
    "\"\"\"\n",
    "!pip install spacy langdetect pdfplumber tqdm pytesseract pdf2image wordcloud seaborn matplotlib xlsxwriter\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_lg\n",
    "!python -m spacy download fr_core_news_sm\n",
    "!python -m spacy download es_core_news_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ff94c5-50f4-40ef-b1b1-f9d421b830e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import Libraries\n",
    "import os\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "import pdfplumber\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import tempfile\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "print(\"All libraries successfully imported!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d4b70d-ed8e-44ea-9993-0ed1b8574be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configure Entity Types and Functions\n",
    "SELECTED_ENTITY_TYPES = [\n",
    "    'PER',      # People's names\n",
    "#    'ORG',      # Organizations\n",
    "#    'GPE',      # Countries, cities, states\n",
    "#    'LOC',      # Non-GPE locations\n",
    "#    'DATE',     # Dates\n",
    "]\n",
    "\n",
    "def is_valid_person_name(text):\n",
    "    \"\"\"\n",
    "    Enhanced check if the extracted text is likely to be a person name\n",
    "    \"\"\"\n",
    "    # Common German words that are often falsely recognized as names\n",
    "    false_positives = {\n",
    "        'Amen', 'Gottes', 'Evangelisationen', 'Talare', 'Wandspr√ºche',\n",
    "        'Korrespondenzkursen', 'Kindruck', 'Offbg', 'Ach',\n",
    "        # Add more false positives as needed\n",
    "    }\n",
    "    \n",
    "    # Normalize text for checking\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Basic validations\n",
    "    if len(text) < 2:\n",
    "        return False\n",
    "    \n",
    "    if text in false_positives:\n",
    "        return False\n",
    "    \n",
    "    if any(char.isdigit() for char in text):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def normalize_german_name(name):\n",
    "    \"\"\"\n",
    "    Normalize German names with improved case handling\n",
    "    \"\"\"\n",
    "    # Dictionary of known name variations (all in lowercase for comparison)\n",
    "    name_variations = {\n",
    "        'luthers': 'Luther',\n",
    "        'luther': 'Luther',\n",
    "        # Add more variations as needed\n",
    "    }\n",
    "    \n",
    "    # Clean and standardize the name\n",
    "    cleaned_name = name.strip().lower()\n",
    "    \n",
    "    # Check if this is a known variation\n",
    "    if cleaned_name in name_variations:\n",
    "        return name_variations[cleaned_name]\n",
    "    \n",
    "    return name\n",
    "\n",
    "class GermanNameProcessor:\n",
    "    def __init__(self):\n",
    "        self.known_names = set()\n",
    "        self.name_variations = {\n",
    "            'luthers': 'Luther',\n",
    "            'luther': 'Luther',\n",
    "            # Add more variations as needed\n",
    "        }\n",
    "    \n",
    "    def process_name(self, name, context=\"\"):\n",
    "        \"\"\"\n",
    "        Process a German name with improved normalization\n",
    "        \"\"\"\n",
    "        if not is_valid_person_name(name):\n",
    "            return None\n",
    "            \n",
    "        if name.lower() in self.name_variations:\n",
    "            return self.name_variations[name.lower()]\n",
    "            \n",
    "        if name in self.known_names:\n",
    "            return name\n",
    "            \n",
    "        # If passed all validations, add to known names\n",
    "        self.known_names.add(name)\n",
    "        return name\n",
    "\n",
    "# Define Text Extraction Function\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file using pdfplumber\n",
    "    Returns the extracted text as a string\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() or \"\"\n",
    "        # print(f\"\\nSample of extracted text:\\n{text[:500]}...\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "# Define Language Detection and Model Loading Function\n",
    "def load_spacy_model(text):\n",
    "    \"\"\"\n",
    "    Detects the language of the text and loads appropriate spaCy model\n",
    "    Returns the loaded model and detected language\n",
    "    \"\"\"\n",
    "    # Function code here (same as in original)\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        \n",
    "        # Map detected language to spaCy model\n",
    "        lang_models = {\n",
    "            'en': 'en_core_web_sm',\n",
    "            'de': 'de_core_news_lg',  # Changed to larger German model\n",
    "            'fr': 'fr_core_news_sm',\n",
    "            'es': 'es_core_news_sm',\n",
    "        }\n",
    "        \n",
    "        if lang in lang_models:\n",
    "            model_name = lang_models[lang]\n",
    "            try:\n",
    "                nlp = spacy.load(model_name)\n",
    "                # For German text, add some pipeline customizations\n",
    "                if lang == 'de':\n",
    "                    nlp.max_length = 2000000  # Increase max text length\n",
    "            except OSError:\n",
    "                print(f\"Downloading language model for {lang}...\")\n",
    "                spacy.cli.download(model_name)\n",
    "                nlp = spacy.load(model_name)\n",
    "            return nlp, lang\n",
    "        else:\n",
    "            print(f\"No available spaCy model for detected language: {lang}\")\n",
    "            return None, lang\n",
    "    except Exception as e:\n",
    "        print(f\"Error in language detection: {str(e)}\")\n",
    "        return None, None\n",
    "    \n",
    "def process_entities(doc, detected_lang=\"de\"):\n",
    "    \"\"\"\n",
    "    Processes entities in the document with improved name normalization\n",
    "    \"\"\"\n",
    "    name_processor = GermanNameProcessor()\n",
    "    entity_counts = defaultdict(Counter)\n",
    "    entities_list = []\n",
    "    \n",
    "    # Create a mapping to track normalized names\n",
    "    normalized_entities = {}\n",
    "    \n",
    "    # Count only selected entity types\n",
    "    selected_entities = [ent for ent in doc.ents if ent.label_ in SELECTED_ENTITY_TYPES]\n",
    "    print(f\"\\nTotal {', '.join(SELECTED_ENTITY_TYPES)} entities found: {len(selected_entities)}\")\n",
    "    \n",
    "    for ent in selected_entities:\n",
    "        if ent.label_ == 'PER' and detected_lang == 'de':\n",
    "            # Get context window\n",
    "            start_idx = max(0, ent.start_char - 100)\n",
    "            end_idx = min(len(doc.text), ent.end_char + 100)\n",
    "            context = doc.text[start_idx:end_idx].replace('\\n', ' ').strip()\n",
    "            \n",
    "            # Process German person names\n",
    "            processed_name = name_processor.process_name(ent.text, context)\n",
    "            if not processed_name:\n",
    "                continue\n",
    "            \n",
    "            # Normalize the name\n",
    "            entity_text = normalize_german_name(processed_name)\n",
    "            \n",
    "            # Store the original-normalized name mapping\n",
    "            normalized_entities[ent.text] = entity_text\n",
    "        else:\n",
    "            entity_text = ent.text\n",
    "        \n",
    "        # Debug: Print only selected entity types\n",
    "        # print(f\"Found {ent.label_}: {entity_text} (original: {ent.text})\")\n",
    "        \n",
    "        entity_detail = {\n",
    "            'text': entity_text,\n",
    "            'original_text': ent.text,\n",
    "            'label': ent.label_,\n",
    "            'start': ent.start_char,\n",
    "            'end': ent.end_char,\n",
    "            'context': context if 'context' in locals() else doc.text[max(0, ent.start_char - 100):min(len(doc.text), ent.end_char + 100)].replace('\\n', ' ').strip()\n",
    "        }\n",
    "        entities_list.append(entity_detail)\n",
    "        entity_counts[ent.label_][entity_text] += 1\n",
    "    \n",
    "    # Print normalization summary\n",
    "    # print(\"\\nName Normalization Summary:\")\n",
    "    # print(\"=\" * 40)\n",
    "    # for original, normalized in normalized_entities.items():\n",
    "    #    if original != normalized:\n",
    "    #        print(f\"'{original}' -> '{normalized}'\")\n",
    "    \n",
    "    return entity_counts, entities_list\n",
    "\n",
    "def analyze_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Main function to analyze a PDF file\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing: {pdf_path}\")\n",
    "    \n",
    "    # Extract text\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    if not text.strip():\n",
    "        print(\"No text could be extracted from PDF\")\n",
    "        return None\n",
    "    \n",
    "    # Load model\n",
    "    nlp, detected_lang = load_spacy_model(text)\n",
    "    if nlp is None:\n",
    "        return None\n",
    "    \n",
    "    print(f\"Detected language: {detected_lang}\")\n",
    "    \n",
    "    # Process text in chunks to handle large documents\n",
    "    max_length = 100000\n",
    "    texts = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "    \n",
    "    all_entity_counts = defaultdict(Counter)\n",
    "    all_entities_list = []\n",
    "    \n",
    "    for text_chunk in texts:\n",
    "        doc = nlp(text_chunk)\n",
    "        entity_counts, entities_list = process_entities(doc, detected_lang)\n",
    "        \n",
    "        # Merge results\n",
    "        for label, counts in entity_counts.items():\n",
    "            all_entity_counts[label].update(counts)\n",
    "        all_entities_list.extend(entities_list)\n",
    "    \n",
    "    return {\n",
    "        'language': detected_lang,\n",
    "        'entity_counts': all_entity_counts,\n",
    "        'entities_list': all_entities_list\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d1b4f-4d0c-4406-96fd-9c9ec98cf8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4:\n",
    "def visualize_top_entities(df_entities):\n",
    "    \"\"\"\n",
    "    Visualize top 10 entities with normalization information\n",
    "    \"\"\"\n",
    "    # Calculate top 10 entities\n",
    "    top_entities = df_entities.groupby('Entity')['Count'].sum().sort_values(ascending=False).head(10)\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "    \n",
    "    # Bar plot\n",
    "    sns.barplot(x=top_entities.values, y=top_entities.index, ax=ax1, palette='viridis')\n",
    "    ax1.set_title('Top 10 Most Frequent Entities (Normalized)', pad=20, size=14)\n",
    "    ax1.set_xlabel('Frequency')\n",
    "    ax1.set_ylabel('Entity')\n",
    "    \n",
    "    # Word Cloud\n",
    "    word_freq = df_entities.groupby('Entity')['Count'].sum().to_dict()\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color='white',\n",
    "        max_words=50,\n",
    "        prefer_horizontal=0.7\n",
    "    ).generate_from_frequencies(word_freq)\n",
    "    \n",
    "    ax2.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('Entity Word Cloud (Normalized)', pad=20, size=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary including original forms\n",
    "    print(\"\\nTop 10 Most Frequent Entities (with original forms):\")\n",
    "    print(\"=\" * 60)\n",
    "    for entity, count in top_entities.items():\n",
    "        original_forms = df_entities[df_entities['Entity'] == entity]['Original_Entity'].unique()\n",
    "        print(f\"{entity}: {count} (Original forms: {', '.join(original_forms)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3a364f-c4c9-440d-aa6d-47547c202a95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 5: Enhanced Visualization Functions for Year Analysis\n",
    "def extract_year_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract year from filename format like \"1947-xxx-xxx.pdf\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract year from the start of the filename\n",
    "        year = filename.split('-')[0]\n",
    "        return int(year)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def visualize_entities_by_year(df_entities):\n",
    "    \"\"\"\n",
    "    Visualize entities grouped by year with both bar plots and word clouds\n",
    "    \"\"\"\n",
    "    # Extract year from PDF_File column\n",
    "    df_entities['Year'] = df_entities['PDF_File'].apply(extract_year_from_filename)\n",
    "    \n",
    "    # Group by Year and Entity, sum the counts\n",
    "    yearly_entities = df_entities.groupby(['Year', 'Entity'])['Count'].sum().reset_index()\n",
    "    \n",
    "    # Get unique years\n",
    "    years = sorted(yearly_entities['Year'].unique())\n",
    "    \n",
    "    # Create a figure with subplots: one row for bar plots, one for word clouds\n",
    "    fig = plt.figure(figsize=(20, 10 * len(years)))\n",
    "    gs = fig.add_gridspec(len(years), 2)\n",
    "    \n",
    "    for idx, year in enumerate(years):\n",
    "        # Filter data for this year\n",
    "        year_data = yearly_entities[yearly_entities['Year'] == year]\n",
    "        top_entities = year_data.nlargest(10, 'Count')\n",
    "        \n",
    "        # Create bar plot for this year\n",
    "        ax_bar = fig.add_subplot(gs[idx, 0])\n",
    "        sns.barplot(data=top_entities, x='Count', y='Entity', ax=ax_bar, palette='viridis')\n",
    "        ax_bar.set_title(f'Top 10 Entities in {year}', pad=20, size=14)\n",
    "        ax_bar.set_xlabel('Frequency')\n",
    "        ax_bar.set_ylabel('Entity')\n",
    "        \n",
    "        # Create word cloud for this year\n",
    "        ax_cloud = fig.add_subplot(gs[idx, 1])\n",
    "        word_freq = year_data.set_index('Entity')['Count'].to_dict()\n",
    "        \n",
    "        if word_freq:  # Only create word cloud if there are entities\n",
    "            wordcloud = WordCloud(\n",
    "                width=800,\n",
    "                height=400,\n",
    "                background_color='white',\n",
    "                max_words=50,\n",
    "                prefer_horizontal=0.7\n",
    "            ).generate_from_frequencies(word_freq)\n",
    "            \n",
    "            ax_cloud.imshow(wordcloud, interpolation='bilinear')\n",
    "            ax_cloud.set_title(f'Entity Word Cloud - {year}', pad=20, size=14)\n",
    "        ax_cloud.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\nTop Entities by Year:\")\n",
    "    print(\"=\" * 60)\n",
    "    for year in years:\n",
    "        year_data = yearly_entities[yearly_entities['Year'] == year]\n",
    "        top_5 = year_data.nlargest(5, 'Count')\n",
    "        print(f\"\\nYear {year}:\")\n",
    "        print(\"-\" * 30)\n",
    "        for _, row in top_5.iterrows():\n",
    "            print(f\"{row['Entity']}: {row['Count']}\")\n",
    "\n",
    "# Single Main Function\n",
    "def main():\n",
    "    # Set your PDF directory path\n",
    "    pdf_directory = \"./data\"  # Replace with your path\n",
    "    \n",
    "    # Create output directory\n",
    "    output_directory = os.path.join(pdf_directory, 'analysis_results')\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    # Process all PDF files\n",
    "    results = {}\n",
    "    pdf_files = [f for f in os.listdir(pdf_directory) if f.endswith('.pdf')]\n",
    "\n",
    "    for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        pdf_path = os.path.join(pdf_directory, pdf_file)\n",
    "        results[pdf_file] = analyze_pdf(pdf_path)\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nEntity Analysis Results\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    all_entities = []\n",
    "    \n",
    "    for pdf_file, result in results.items():\n",
    "        if result is None:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nFile: {pdf_file}\")\n",
    "        print(f\"Language: {result['language']}\")\n",
    "        \n",
    "        if not result['entity_counts']:\n",
    "            print(\"No entities found in this document.\")\n",
    "            continue\n",
    "        \n",
    "        for entity_type, counts in result['entity_counts'].items():\n",
    "            print(f\"\\n{entity_type} entities found: {sum(counts.values())}\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            sorted_entities = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for entity, count in sorted_entities[:10]:\n",
    "                print(f\"{entity}: {count}\")\n",
    "                \n",
    "        for entity in result['entities_list']:\n",
    "            all_entities.append({\n",
    "                'PDF_File': pdf_file,\n",
    "                'Entity': entity['text'],\n",
    "                'Type': entity['label'],\n",
    "                'Count': result['entity_counts'][entity['label']][entity['text']],\n",
    "                'Context': entity['context'],\n",
    "                'Language': result['language']\n",
    "            })\n",
    "\n",
    "    if all_entities:\n",
    "        df_entities = pd.DataFrame(all_entities)\n",
    "        \n",
    "        # Display visualizations by year\n",
    "        print(\"\\nGenerating visualizations by year...\")\n",
    "        visualize_entities_by_year(df_entities)\n",
    "        \n",
    "        # Save to Excel\n",
    "        output_directory = os.path.join(pdf_directory, 'analysis_results')\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        excel_path = os.path.join(output_directory, 'named_entities_analysis.xlsx')\n",
    "        \n",
    "        with pd.ExcelWriter(excel_path, engine='xlsxwriter') as writer:\n",
    "            # Summary sheet with year grouping\n",
    "            df_summary = df_entities.groupby(['Year', 'Type', 'Entity'])['Count'].sum().reset_index()\n",
    "            df_summary = df_summary.sort_values(['Year', 'Type', 'Count'], ascending=[True, True, False])\n",
    "            df_summary.to_excel(writer, sheet_name='Summary', index=False)\n",
    "            \n",
    "            # Detailed sheet\n",
    "            df_entities.to_excel(writer, sheet_name='Detailed', index=False)\n",
    "            \n",
    "            # Entity type sheets by year\n",
    "            for entity_type in SELECTED_ENTITY_TYPES:\n",
    "                df_type = df_entities[df_entities['Type'] == entity_type]\n",
    "                if not df_type.empty:\n",
    "                    df_type.to_excel(writer, sheet_name=f'{entity_type[:30]}', index=False)\n",
    "            \n",
    "            # Year-specific sheets\n",
    "            for year in df_entities['Year'].unique():\n",
    "                df_year = df_entities[df_entities['Year'] == year]\n",
    "                if not df_year.empty:\n",
    "                    df_year.to_excel(writer, sheet_name=f'Year_{year}', index=False)\n",
    "        \n",
    "        print(f\"\\nDetailed results saved to: {excel_path}\")\n",
    "    else:\n",
    "        print(\"\\nNo entities were found in any of the documents.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa5407e-9802-4d54-a99e-075e0077735c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e2e88-6916-4263-8353-3b90e26187f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
